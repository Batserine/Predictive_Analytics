{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon reviews multilingual UK dataset\n",
    "\n",
    "\n",
    "This is divided into 4 tasks:\n",
    "1. __Data Processing__ \n",
    "2. __Classification__ \n",
    "3. __Regression__ \n",
    "4. __Recommender Sytstems__\n",
    "    1. Similarity matching\n",
    "    2. Predictions\n",
    "    3. Recommendations on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Data Processing\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this project I will be doing on amazon reviews dataset. The list of such dataset repository can be found [here.](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "This dataset is a set of multiple Product reviews bought in UK on amazon. This dataset is of size ~333 MB, so its a mid-range dataset.\n",
    "\n",
    "### First Step: Imports\n",
    "\n",
    "Importing all necessary libraries needed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import string\n",
    "import nltk\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Read the data and Fill your dataset\n",
    "\n",
    "1. Type Casting some of the features.\n",
    "2. Converting any boolean responses to True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"amazon_reviews_multilingual_UK_v1_00.tsv.gz\"\n",
    "\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "\n",
    "# print(header)\n",
    "dataset = []\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    for field in ['verified_purchase','vine']:\n",
    "        if d[field] == 'Y':\n",
    "            d[field]=True\n",
    "        else:\n",
    "            d[field]=False\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marketplace': 'UK',\n",
       " 'customer_id': '28026896',\n",
       " 'review_id': 'R4CP7B77ADSJ3',\n",
       " 'product_id': 'B003TML0VO',\n",
       " 'product_parent': '838418618',\n",
       " 'product_title': 'Guitar Heaven: Santana Performs The Greatest Guitar Classics Of All Time',\n",
       " 'product_category': 'Music',\n",
       " 'star_rating': 2,\n",
       " 'helpful_votes': 0,\n",
       " 'total_votes': 0,\n",
       " 'vine': False,\n",
       " 'verified_purchase': True,\n",
       " 'review_headline': 'Ok',\n",
       " 'review_body': 'Ok have bought better.',\n",
       " 'review_date': '2015-01-18'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Split the data into a Training and Testing set\n",
    "\n",
    "Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  1365995 Test Set:  341499 Total no.of rows 1707494\n"
     ]
    }
   ],
   "source": [
    "#2107824 526957\n",
    "# Lengths should be: 2107824 526957\n",
    "random.shuffle(dataset)\n",
    "\n",
    "N = len(dataset)\n",
    "trainingSet = dataset[:4*N//5]\n",
    "testingSet = dataset[4*N//5:]\n",
    "\n",
    "print(\"Training Set: \",len(trainingSet), \"Test Set: \",len(testingSet), \"Total no.of rows\",N)\n",
    "# print(\"Lengths should be: 2107824 526957\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Extracting Basic Statistics\n",
    "\n",
    "Next calculate the answer to some statistic questions all based on the __Training Set:__\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  4.379938433156783\n",
      "2.  76.2219481037632\n",
      "3.  797681\n",
      "4.  54954\n",
      "5.  67.1279177449405\n"
     ]
    }
   ],
   "source": [
    "d_star = [d['star_rating'] for d in trainingSet]\n",
    "avg_rating = np.average(d_star)\n",
    "print(\"1. \",avg_rating)\n",
    "\n",
    "d_ver = [d['verified_purchase'] for d in trainingSet if d['verified_purchase'] ==True ]\n",
    "frac_reviews = (len(d_ver)/len(trainingSet))*100\n",
    "print(\"2. \",frac_reviews)\n",
    "\n",
    "# This way it takes unique customer_id and product_id\n",
    "users = set()\n",
    "for d in trainingSet:\n",
    "    users.add(d['customer_id'])\n",
    "print(\"3. \",len(users))\n",
    "items = set()\n",
    "for d in trainingSet:\n",
    "    items.add(d['product_id'])\n",
    "print(\"4. \",len(items))\n",
    "\n",
    "d_five = [d['star_rating'] for d in trainingSet if d['star_rating'] ==5 ]\n",
    "frac_five = (len(d_five)/len(trainingSet))*100\n",
    "print(\"5. \",frac_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Classification\n",
    "\n",
    "Perform classification to extract features and make predictions based on them. Here I will be using a Logistic Regression Model.\n",
    "\n",
    "### 1: Define the feature function\n",
    "\n",
    "This implementation will be based on the __star rating__ and the ___length___ of the __review body__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "# wordCount = defaultdict(int)\n",
    "# punctuation = set(string.punctuation)\n",
    "\n",
    "# #GIVEN for 2.\n",
    "# # wordCountStem = defaultdict(int)\n",
    "#  print(len(wordCount))\n",
    "\n",
    "# counts = [(wordCount[w],w) for w in wordCount]\n",
    "# words = [x[1] for x in counts]\n",
    "# wordid = dict(zip(words,range(len(words))))\n",
    "# for d in dataset:\n",
    "#     f = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "#     for w in r.split():\n",
    "#         w = stemmer.stem(w) # with stemming\n",
    "#         wordCount[w] += 1\n",
    "# stemmer.stem()\n",
    "#     features = [0]*len(words)\n",
    "#     global f\n",
    "#     for w in f.split():\n",
    "#         if w in words:\n",
    "#             features[wordid[w]]+=1\n",
    "#     features.append(1)\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)\n",
    "for d in trainingSet:\n",
    "    f = ''.join([x for x in d['review_body'].lower() if not x in string.punctuation])\n",
    "for w in f.split():\n",
    "    w = stemmer.stem(w) # with stemming\n",
    "    wordCount[w]+=1\n",
    "\n",
    "\n",
    "def feature(dat):\n",
    "    feat = [1, dat['star_rating'], len(wordCount)]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Fit your model\n",
    "\n",
    "1. Creating a __Feature Vector__ based on the feature function defined above. \n",
    "2. Creating a __Label Vector__ based on the \"verified purchase\" column from the training set.\n",
    "3. Defining a model i.e; __Logistic Regression__ model.\n",
    "4. Fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [feature(d) for d in trainingSet]\n",
    "y = [d['verified_purchase'] for d in trainingSet]\n",
    "\n",
    "# print(\"Label: \", y[:100], \"\\nFeatures:\", X[:10])\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on the model.\n",
    "2. Compute the __Accuracy__ of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761759742898034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE without stemming: 0.7619478841430606  with stemming: 0.7619478841430606\n",
    "predictions = model.predict(X)\n",
    "# predictions\n",
    "correctPredictions = predictions == y\n",
    "accuracy = sum(correctPredictions) / len(correctPredictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4: Finding the Balanced Error Rate\n",
    "\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on the above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 1040560 \n",
      "FP: 325435 \n",
      "TN: 0 \n",
      "FN: 0 \n",
      "TF accuracy(should be equal to above accuracy): 0.761759742898034 \n",
      "Balanced Error rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "#YOUR CODE HERE\n",
    "TP = sum([(p and l) for (p,l) in zip(predictions, y)])\n",
    "FP = sum([(p and not l) for (p,l) in zip(predictions, y)])\n",
    "TN = sum([(not p and not l) for (p,l) in zip(predictions, y)])\n",
    "FN = sum([(not p and l) for (p,l) in zip(predictions, y)])\n",
    "\n",
    "TFaccuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "TPR = TP / (TP + FN)\n",
    "TNR = TN / (TN + FP)\n",
    "BER = 1 - 1/2 * (TPR + TNR)\n",
    "print(\"TP:\",TP,\"\\nFP:\",FP,\"\\nTN:\",TN,\"\\nFN:\",FN,\"\\nTF accuracy(should be equal to above accuracy):\",TFaccuracy,\"\\nBalanced Error rate:\",BER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Regression\n",
    "\n",
    "Alter the features to differentiate. \n",
    "\n",
    "Here I will be using word ID's and star rating as feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d['star_rating'] for d in trainingSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Unique Words in a Sample Set\n",
    "\n",
    "I will take a smaller Sample Set here, as stemming on the normal training set will take a very long time.\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSet = trainingSet[:2*len(trainingSet)//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordCount: ['great', 'cd'] 2 \n",
      "wordStem Count: ['great', 'cd'] 2\n"
     ]
    }
   ],
   "source": [
    "for d in sampleSet:\n",
    "    f = ''.join([x for x in d['review_body'].lower() if not x in punctuation])\n",
    "for w in f.split():\n",
    "    w = stemmer.stem(w) # with stemming\n",
    "    wordCountStem[w]+=1\n",
    "for w in f.split():\n",
    "#     w = stemmer.stem(w) # with stemming\n",
    "    wordCount[w]+=1\n",
    "    \n",
    "counts = [(wordCount[w],w) for w in wordCount]\n",
    "counts_stem = [(wordCountStem[w],w) for w in wordCountStem]\n",
    "\n",
    "words = [x[1] for x in counts]\n",
    "words_stem = [x[1] for x in counts_stem]\n",
    "print(\"wordCount:\",words,len(words),\"\\nwordStem Count:\",words_stem,len(words_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and counts vector, __Define__ a X vector.\n",
    "2. __Fit__ the model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using the model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between resulted predictions and y vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN FUNCTIONS\n",
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1852399967940734"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(trainingSet)\n",
    "X = [feature_reg(d) for d in trainingSet]\n",
    "\n",
    "model = linear_model.Ridge(alpha = 1.0, fit_intercept = True)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "def MSE(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    differences = [(a-b)**2 for (a,b) in zip(predictions, y)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "MSE(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with this example more in your free time, here are some tips to improve your solution:\n",
    "# 1. Implement a validation pipeline and tune the regularization parameter\n",
    "# 2. Alter the word features (e.g. dictionary size, punctuation, capitalization, stemming, etc.)\n",
    "# 3. Incorporate features other than word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Recommendation Systems\n",
    "\n",
    "For this final task, you will see a simple latent factor-based recommender systems to make predictions. Then evaluating the performance of this predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fill our default dictionaries for our dataset\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in trainingSet:\n",
    "    user,item = d['customer_id'], d['product_id']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    \n",
    "#Create two dictionaries that will be filled with our rating prediction values\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "#Getting the respective lengths of our dataset and dictionaries\n",
    "N = len(trainingSet)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "\n",
    "#Getting the list of keys\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())\n",
    "\n",
    "labels = [d['star_rating'] for d in trainingSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of the training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating mean:  4.379938433156783\n",
      "MSE:  1.1841831286457427\n"
     ]
    }
   ],
   "source": [
    "alpha = sum([d['star_rating'] for d in trainingSet]) / len(trainingSet)\n",
    "# alpha = np.reshape(-1,1)\n",
    "alwaysPredictMean = [alpha for d in dataset]\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "# labels = [d['star_rating'] for d in trainingSet]\n",
    "# print(labels[:100])\n",
    "print(\"Rating mean: \",alpha)\n",
    "print(\"MSE: \",MSE(alwaysPredictMean, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "K = 2 #Dimensionality of gamma\n",
    "\n",
    "for u in reviewsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    \n",
    "for i in reviewsPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions defined to optimize the above MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = ratingMean\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K\n",
    "        \n",
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])\n",
    "\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item] + inner(userGamma[user], itemGamma[item])\n",
    "\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['customer_id'], d['product_id']) for d in trainingSet]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost\n",
    "\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(trainingSet)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in reviewsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in reviewsPerItem:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for d in trainingSet:\n",
    "        u,i = d['customer_id'], d['product_id']\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d['star_rating']\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Optimize\n",
    "\n",
    "1. __Optimize__ the above MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.1841849224291974\n",
      "MSE = 1.181288667550596\n",
      "MSE = 1.1710962556469446\n",
      "MSE = 101.85112437814225\n",
      "MSE = 1.1878040176514137\n",
      "MSE = 1.1644660269237566\n",
      "MSE = 1.1380740009422736\n",
      "MSE = 1.137450892993197\n",
      "MSE = 1.1383953066158647\n",
      "MSE = 1.1402806036412134\n",
      "MSE = 1.140799705830948\n",
      "MSE = 1.1410790537243674\n",
      "MSE = 1.1411118599148247\n",
      "MSE = 1.1411078990215409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.38379472e+00,  9.09613047e-04, -3.19293241e-04, ...,\n",
       "         1.79226023e-07,  1.98776919e-07, -3.33638325e-07]),\n",
       " 1.1574362235405544,\n",
       " {'grad': array([ 2.29508812e-06, -3.51997289e-10, -3.12451242e-10, ...,\n",
       "          3.66167954e-10,  3.97248128e-10, -6.66952398e-10]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 14,\n",
       "  'nit': 11,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + # Initialize alpha\n",
    "                                   [0.0]*(nUsers+nItems) + # Initialize beta\n",
    "                                   [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))], derivative, \n",
    "                             args = (labels, 0.001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Recommending Products\n",
    "\n",
    "    Based on similarities in trainingSet Recommendations were made on TestingSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "itemNames = {}\n",
    "\n",
    "for d in trainingSet:\n",
    "    user,item = d['customer_id'], d['product_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    itemNames[item] = d['product_title']\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def mostSimilar(iD, n):\n",
    "    similarities = []\n",
    "    id_list = []\n",
    "    users = usersPerItem[iD]\n",
    "    for i2 in usersPerItem:\n",
    "        if i2 == iD: continue\n",
    "        sim = Jaccard(users, usersPerItem[i2])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    for i in similarities:\n",
    "        id_list.append(i[1])\n",
    "    print(id_list[:n])\n",
    "    return similarities[:n]\n",
    "\n",
    "# def predictRating(user,item):\n",
    "#     ratings = []\n",
    "#     similarities = []\n",
    "#     for d in reviewsPerUser[user]:\n",
    "#         i2 = d['product_id']\n",
    "#         if i2 == item: continue\n",
    "#         ratings.append(d['star_rating'])\n",
    "#         similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "#     if (sum(similarities) > 0):\n",
    "#         weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "#         return sum(weightedRatings) / sum(similarities)\n",
    "#     else:\n",
    "#         # User hasn't rated any similar items\n",
    "#         return ratingMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0330517945\n"
     ]
    }
   ],
   "source": [
    "query = testingSet[10]['product_id']\n",
    "# query1 = testingSet['product_id']\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0330517937', '0330517953', '0230748260', '0230748252', '0330419080', '0330419072', '0330419099', 'B004XBOANU', '0230748236', '0857207539']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.15421686746987953, '0330517937'),\n",
       " (0.07736389684813753, '0330517953'),\n",
       " (0.019390581717451522, '0230748260'),\n",
       " (0.0160857908847185, '0230748252'),\n",
       " (0.012448132780082987, '0330419080'),\n",
       " (0.011811023622047244, '0330419072'),\n",
       " (0.008658008658008658, '0330419099'),\n",
       " (0.007782101167315175, 'B004XBOANU'),\n",
       " (0.006734006734006734, '0230748236'),\n",
       " (0.00641025641025641, '0857207539')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostSimilar(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0330517937', '0330517953', '0230748260', '0230748252', '0330419080', '0330419072', '0330419099', 'B004XBOANU', '0230748236', '0857207539']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The Sins of the Father (The Clifton Chronicles)',\n",
       " 'Be Careful What You Wish For (The Clifton Chronicles)',\n",
       " 'Mightier than the Sword (The Clifton Chronicles)',\n",
       " 'Be Careful What You Wish For (The Clifton Chronicles)',\n",
       " 'The Fourth Estate',\n",
       " 'The Eleventh Commandment',\n",
       " 'To Cut A Long Story Short',\n",
       " 'Little Voice [DVD]',\n",
       " 'The Sins of the Father (The Clifton Chronicles)',\n",
       " \"The White Princess (COUSINS' WAR)\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itemNames[x[1]] for x in mostSimilar(query, 10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
