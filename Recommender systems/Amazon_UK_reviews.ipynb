{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon reviews multilingual UK dataset\n",
    "\n",
    "\n",
    "This is divided into 4 tasks:\n",
    "1. __Data Processing__ \n",
    "2. __Classification__ \n",
    "3. __Regression__ \n",
    "4. __Recommender Sytstems__\n",
    "    1. Similarity matching\n",
    "    2. Predictions\n",
    "    3. Recommendations on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Data Processing\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this project I will be doing on amazon reviews dataset. The list of such dataset repository can be found [here.](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)\n",
    "This dataset is a set of multiple Product reviews bought in UK on amazon. This dataset is of size ~333 MB, so its a mid-range dataset.\n",
    "\n",
    "### DATA COLUMNS:\n",
    "    marketplace       - 2 letter country code of the marketplace where the review was written.\n",
    "    customer_id       - Random identifier that can be used to aggregate reviews written by a single author.\n",
    "    review_id         - The unique ID of the review.\n",
    "    product_id        - The unique Product ID the review pertains to. In the multilingual dataset the reviews\n",
    "                    for the same product in different countries can be grouped by the same product_id.\n",
    "    product_parent    - Random identifier that can be used to aggregate reviews for the same product.\n",
    "    product_title     - Title of the product.\n",
    "    product_category  - Broad product category that can be used to group reviews \n",
    "                    (also used to group the dataset into coherent parts).\n",
    "    star_rating       - The 1-5 star rating of the review.\n",
    "    helpful_votes     - Number of helpful votes.\n",
    "    total_votes       - Number of total votes the review received.\n",
    "    vine              - Review was written as part of the Vine program.\n",
    "    verified_purchase - The review is on a verified purchase.\n",
    "    review_headline   - The title of the review.\n",
    "    review_body       - The review text.\n",
    "    review_date       - The date the review was written.\n",
    "\n",
    "### DATA FORMAT\n",
    "    Tab ('\\t') separated text file, without quote or escape characters.\n",
    "    First line in each file is header; 1 line corresponds to 1 record.\n",
    "\n",
    "### First Step: Imports\n",
    "\n",
    "Importing all necessary libraries needed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import string\n",
    "import nltk\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Read the data and Fill your dataset\n",
    "\n",
    "1. Type Casting some of the features.\n",
    "2. Converting any boolean responses to True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"amazon_reviews_multilingual_UK_v1_00.tsv.gz\"\n",
    "\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')\n",
    "\n",
    "# print(header)\n",
    "dataset = []\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    for field in ['verified_purchase','vine']:\n",
    "        if d[field] == 'Y':\n",
    "            d[field]=True\n",
    "        else:\n",
    "            d[field]=False\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'marketplace': 'UK',\n",
       " 'customer_id': '17809',\n",
       " 'review_id': 'R1X8DIPIAIWFK9',\n",
       " 'product_id': 'B00AESN8XY',\n",
       " 'product_parent': '593367180',\n",
       " 'product_title': 'Plague Inc.',\n",
       " 'product_category': 'Mobile_Apps',\n",
       " 'star_rating': 4,\n",
       " 'helpful_votes': 0,\n",
       " 'total_votes': 0,\n",
       " 'vine': False,\n",
       " 'verified_purchase': True,\n",
       " 'review_headline': 'gd',\n",
       " 'review_body': 'Good game I spose',\n",
       " 'review_date': '2015-03-11'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Split the data into a Training and Testing set\n",
    "\n",
    "Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:  1365995 \n",
      "Test Set:  341499 \n",
      "Total no.of rows 1707494\n"
     ]
    }
   ],
   "source": [
    "#2107824 526957\n",
    "# Lengths should be: 2107824 526957\n",
    "random.shuffle(dataset)\n",
    "\n",
    "N = len(dataset)\n",
    "trainingSet = dataset[:4*N//5]\n",
    "testingSet = dataset[4*N//5:]\n",
    "\n",
    "print(\"Training Set: \",len(trainingSet), \"\\nTest Set: \",len(testingSet), \"\\nTotal no.of rows\",N)\n",
    "# print(\"Lengths should be: 2107824 526957\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Extracting Basic Statistics\n",
    "\n",
    "Next calculate the answer to some statistic questions all based on the __Training Set:__\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  4.379450144400236\n",
      "2.  76.17 %\n",
      "3.  797423\n",
      "4.  55021\n",
      "5.  67.1 %\n"
     ]
    }
   ],
   "source": [
    "d_star = [d['star_rating'] for d in trainingSet]\n",
    "avg_rating = np.average(d_star)\n",
    "print(\"1. \",avg_rating)\n",
    "\n",
    "d_ver = [d['verified_purchase'] for d in trainingSet if d['verified_purchase'] ==True ]\n",
    "frac_reviews = (len(d_ver)/len(trainingSet))*100\n",
    "print(\"2. \",round(frac_reviews,2),\"%\")\n",
    "\n",
    "# This way it takes unique customer_id and product_id\n",
    "users = set()\n",
    "for d in trainingSet:\n",
    "    users.add(d['customer_id'])\n",
    "print(\"3. \",len(users))\n",
    "items = set()\n",
    "for d in trainingSet:\n",
    "    items.add(d['product_id'])\n",
    "print(\"4. \",len(items))\n",
    "\n",
    "d_five = [d['star_rating'] for d in trainingSet if d['star_rating'] ==5 ]\n",
    "frac_five = (len(d_five)/len(trainingSet))*100\n",
    "print(\"5. \",round(frac_five,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Classification\n",
    "\n",
    "Perform classification to extract features and make predictions based on them. Here I will be using a Logistic Regression Model.\n",
    "\n",
    "### 1: Define the feature function\n",
    "\n",
    "This implementation will be based on the __star rating__ and the ___length___ of the __review body__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "# wordCount = defaultdict(int)\n",
    "# punctuation = set(string.punctuation)\n",
    "\n",
    "# #GIVEN for 2.\n",
    "# # wordCountStem = defaultdict(int)\n",
    "#  print(len(wordCount))\n",
    "\n",
    "# counts = [(wordCount[w],w) for w in wordCount]\n",
    "# words = [x[1] for x in counts]\n",
    "# wordid = dict(zip(words,range(len(words))))\n",
    "# for d in dataset:\n",
    "#     f = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "#     for w in r.split():\n",
    "#         w = stemmer.stem(w) # with stemming\n",
    "#         wordCount[w] += 1\n",
    "# stemmer.stem()\n",
    "#     features = [0]*len(words)\n",
    "#     global f\n",
    "#     for w in f.split():\n",
    "#         if w in words:\n",
    "#             features[wordid[w]]+=1\n",
    "#     features.append(1)\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)\n",
    "for d in trainingSet:\n",
    "    f = ''.join([x for x in d['review_body'].lower() if not x in string.punctuation])\n",
    "for w in f.split():\n",
    "    w = stemmer.stem(w) # with stemming\n",
    "    wordCount[w]+=1\n",
    "\n",
    "\n",
    "def feature(dat):\n",
    "    feat = [1, dat['star_rating'], len(wordCount)]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Fit your model\n",
    "\n",
    "1. Creating a __Feature Vector__ based on the feature function defined above. \n",
    "2. Creating a __Label Vector__ based on the \"verified purchase\" column from the training set.\n",
    "3. Defining a model i.e; __Logistic Regression__ model.\n",
    "4. Fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [feature(d) for d in trainingSet]    \n",
    "y_train = [d['verified_purchase'] for d in trainingSet]   \n",
    "\n",
    "X_test = [feature(d) for d in testingSet]    \n",
    "y_test = [d['verified_purchase'] for d in testingSet]   \n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print(\"Label: \", y[:100], \"\\nFeatures:\", X[:10])\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on the model.\n",
    "2. Compute the __Accuracy__ of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  76.21 % \n",
      "Test accuracy:  76.14 %\n",
      "Confusion matrix: \n",
      " [[     0  81466]\n",
      " [     0 260033]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_train = model.predict(X_train_scaled)\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "\n",
    "correctPredictions_train = predictions_train == y_train\n",
    "correctPredictions_test = predictions_test == y_test\n",
    "\n",
    "accuracy_train = sum(correctPredictions_train) / len(correctPredictions_train)*100\n",
    "accuracy_test = sum(correctPredictions_test) / len(correctPredictions_test)*100\n",
    "\n",
    "print(\"Training accuracy: \",round(accuracy_train,2),\"%\",\"\\nTest accuracy: \",round(accuracy_test,2),\"%\")\n",
    "print(\"Confusion matrix: \\n\",confusion_matrix(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4: Finding the Balanced Error Rate\n",
    "\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on the above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP_train = 1041020\n",
      "FP_train = 324975\n",
      "TN_train = 0\n",
      "FN_train = 0\n",
      "TF_Accuracy: 76.21%\n",
      "BER_train = 0.5\n"
     ]
    }
   ],
   "source": [
    "TP_train = sum([(p and l) for (p, l) in zip(predictions_train, y_train)])\n",
    "FP_train = sum([(p and not l) for (p, l) in zip(predictions_train, y_train)])\n",
    "TN_train = sum([(not p and not l) for (p, l) in zip(predictions_train, y_train)])\n",
    "FN_train = sum([(not p and l) for (p, l) in zip(predictions_train, y_train)])\n",
    "TF_accuracy = (TP_train + TN_train) / (TP_train + FP_train + TN_train + FN_train)\n",
    "BER = 1 - 1/2 * (TP_train / (TP_train + FN_train) + TN_train / (TN_train + FP_train))\n",
    "print(f'TP_train = {TP_train}')\n",
    "print(f'FP_train = {FP_train}')\n",
    "print(f'TN_train = {TN_train}')\n",
    "print(f'FN_train = {FN_train}')\n",
    "print(f'TF_Accuracy: {round(TF_accuracy*100,2)}%')\n",
    "print(f'BER_train = {BER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP_train = 260033\n",
      "FP_train = 81466\n",
      "TN_train = 0\n",
      "FN_train = 0\n",
      "TF_Accuracy: 76.14%\n",
      "BER_train = 0.5\n"
     ]
    }
   ],
   "source": [
    "TP_test = sum([(p and l) for (p, l) in zip(predictions_test, y_test)])\n",
    "FP_test = sum([(p and not l) for (p, l) in zip(predictions_test, y_test)])\n",
    "TN_test = sum([(not p and not l) for (p, l) in zip(predictions_test, y_test)])\n",
    "FN_test = sum([(not p and l) for (p, l) in zip(predictions_test, y_test)])\n",
    "TF_accuracy = (TP_test + TN_test) / (TP_test + FP_test + TN_test + FN_test)\n",
    "BER = 1 - 1/2 * (TP_test / (TP_test + FN_test) + TN_test / (TN_test + FP_test))\n",
    "print(f'TP_train = {TP_test}')\n",
    "print(f'FP_train = {FP_test}')\n",
    "print(f'TN_train = {TN_test}')\n",
    "print(f'FN_train = {FN_test}')\n",
    "print(f'TF_Accuracy: {round(TF_accuracy*100,2)}%')\n",
    "print(f'BER_train = {BER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Regression\n",
    "\n",
    "Alter the features to differentiate. \n",
    "\n",
    "Here I will be using word ID's and star rating as feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Unique Words in a Sample Set\n",
    "\n",
    "I will take a smaller Sample Set here, as stemming on the normal training set will take a very long time.\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSet = trainingSet[:2*len(trainingSet)//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordCount: ['excellent'] 1 \n",
      "wordStem Count: ['excel'] 1\n"
     ]
    }
   ],
   "source": [
    "for d in sampleSet:\n",
    "    f = ''.join([x for x in d['review_body'].lower() if not x in punctuation])\n",
    "for w in f.split():\n",
    "    w = stemmer.stem(w) # with stemming\n",
    "    wordCountStem[w]+=1\n",
    "        \n",
    "# for d in trainingSet:\n",
    "#     f = ''.join([x for x in d['review_body'].lower() if not x in punctuation])\n",
    "for w in f.split():\n",
    "    wordCount[w]+=1\n",
    "    \n",
    "counts = [(wordCount[w],w) for w in wordCount]\n",
    "counts_stem = [(wordCountStem[w],w) for w in wordCountStem]\n",
    "\n",
    "words = [x[1] for x in counts]\n",
    "words_stem = [x[1] for x in counts_stem]\n",
    "print(\"wordCount:\",words,len(words),\"\\nwordStem Count:\",words_stem,len(words_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and counts vector, __Define__ a X vector.\n",
    "2. __Fit__ the model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using the model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between resulted predictions and y vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN FUNCTIONS\n",
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  1.1778335377014026\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(trainingSet)\n",
    "X_train_reg = [feature_reg(d) for d in trainingSet]\n",
    "y_train_reg = [d['star_rating'] for d in trainingSet]\n",
    "\n",
    "X_test_reg = [feature_reg(d) for d in testingSet]\n",
    "y_test_reg = [d['star_rating'] for d in testingSet]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_reg)\n",
    "X_train_reg_scaled = scaler.transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler.transform(X_test_reg)\n",
    "\n",
    "model = linear_model.Ridge(alpha = 1.0, fit_intercept = True)\n",
    "model.fit(X_train_reg_scaled, y_train_reg)\n",
    "y_test_pred = model.predict(X_test_reg_scaled)\n",
    "mse = MSE(y_test_pred, y_test_reg)\n",
    "\n",
    "print(\"MSE: \",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with this example more in your free time, here are some tips to improve your solution:\n",
    "# 1. Implement a validation pipeline and tune the regularization parameter\n",
    "# 2. Alter the word features (e.g. dictionary size, punctuation, capitalization, stemming, etc.)\n",
    "# 3. Incorporate features other than word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Recommendation Systems\n",
    "\n",
    "For this final task, you will see a simple latent factor-based recommender systems to make predictions. Then evaluating the performance of this predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fill our default dictionaries for our dataset\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in trainingSet:\n",
    "    user,item = d['customer_id'], d['product_id']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    \n",
    "#Create two dictionaries that will be filled with our rating prediction values\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "#Getting the respective lengths of our dataset and dictionaries\n",
    "N = len(trainingSet)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "\n",
    "#Getting the list of keys\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())\n",
    "\n",
    "labels = [d['star_rating'] for d in trainingSet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of the training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating mean:  4.379450144400236\n",
      "MSE:  1.1851124967710491\n"
     ]
    }
   ],
   "source": [
    "alpha = sum([d['star_rating'] for d in trainingSet]) / len(trainingSet)\n",
    "# alpha = np.reshape(-1,1)\n",
    "alwaysPredictMean = [alpha for d in trainingSet]\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "# labels = [d['star_rating'] for d in trainingSet]\n",
    "# print(labels[:100])\n",
    "print(\"Rating mean: \",alpha)\n",
    "print(\"MSE: \",MSE(alwaysPredictMean, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "K = 2 #Dimensionality of gamma\n",
    "\n",
    "for u in reviewsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    \n",
    "for i in reviewsPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions defined to optimize the above MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = ratingMean\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K\n",
    "        \n",
    "def inner(x, y):\n",
    "    return sum([a*b for a,b in zip(x,y)])\n",
    "\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item] + inner(userGamma[user], itemGamma[item])\n",
    "\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(d['customer_id'], d['product_id']) for d in trainingSet]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost\n",
    "\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(trainingSet)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in reviewsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in reviewsPerItem:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for d in trainingSet:\n",
    "        u,i = d['customer_id'], d['product_id']\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - d['star_rating']\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Optimize\n",
    "\n",
    "1. __Optimize__ the above MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.1851130366731504\n",
      "MSE = 1.1822930372457103\n",
      "MSE = 1.1723415069484748\n",
      "MSE = 106.06406002545341\n",
      "MSE = 1.1883609556190189\n",
      "MSE = 1.1644849173794891\n",
      "MSE = 1.139567901823131\n",
      "MSE = 1.138967496958636\n",
      "MSE = 1.1399467030896961\n",
      "MSE = 1.141787650918011\n",
      "MSE = 1.1422832258314293\n",
      "MSE = 1.1425497236767768\n",
      "MSE = 1.1425707025961862\n",
      "MSE = 1.142571705087677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4.38334481e+00, -4.04016730e-03,  8.21316899e-04, ...,\n",
       "         3.88408651e-07,  1.16490547e-07, -6.08750809e-07]),\n",
       " 1.1587750830270385,\n",
       " {'grad': array([ 1.73857309e-06, -6.50465159e-10, -8.14765130e-11, ...,\n",
       "          7.74118627e-10,  2.33175246e-10, -1.21687998e-09]),\n",
       "  'task': b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 14,\n",
       "  'nit': 11,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + # Initialize alpha\n",
    "                                   [0.0]*(nUsers+nItems) + # Initialize beta\n",
    "                                   [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))], derivative, \n",
    "                             args = (labels, 0.001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Recommending Products\n",
    "\n",
    "    Based on similarities in trainingSet Recommendations were made on TestingSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "itemNames = {}\n",
    "\n",
    "for d in trainingSet:\n",
    "    user,item = d['customer_id'], d['product_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    itemNames[item] = d['product_title']\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "def mostSimilar(iD, n):\n",
    "    similarities = []\n",
    "    id_list = []\n",
    "    users = usersPerItem[iD]\n",
    "    for i2 in usersPerItem:\n",
    "        if i2 == iD: continue\n",
    "        sim = Jaccard(users, usersPerItem[i2])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    for i in similarities:\n",
    "        id_list.append(i[1])\n",
    "    print(id_list[:n])\n",
    "    return similarities[:n]\n",
    "\n",
    "# def predictRating(user,item):\n",
    "#     ratings = []\n",
    "#     similarities = []\n",
    "#     for d in reviewsPerUser[user]:\n",
    "#         i2 = d['product_id']\n",
    "#         if i2 == item: continue\n",
    "#         ratings.append(d['star_rating'])\n",
    "#         similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "#     if (sum(similarities) > 0):\n",
    "#         weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "#         return sum(weightedRatings) / sum(similarities)\n",
    "#     else:\n",
    "#         # User hasn't rated any similar items\n",
    "#         return ratingMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product ID:  1401216676 \n",
      "Product title: Batman The Killing Joke Special Ed HC\n"
     ]
    }
   ],
   "source": [
    "query = testingSet[10]['product_id']\n",
    "# query1 = testingSet['product_id']\n",
    "\n",
    "print(\"Product ID: \",query,\"\\nProduct title:\",itemNames[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1401207529', '1401232590', '1401223176', '1401235425', 'B00X5ZT04E', '140120841X', '1607066017', '160309329X', 'B003N5VTUY', 'B000028D3Y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.07339449541284404, '1401207529'),\n",
       " (0.02912621359223301, '1401232590'),\n",
       " (0.02586206896551724, '1401223176'),\n",
       " (0.024793388429752067, '1401235425'),\n",
       " (0.018691588785046728, 'B00X5ZT04E'),\n",
       " (0.017857142857142856, '140120841X'),\n",
       " (0.013333333333333334, '1607066017'),\n",
       " (0.013157894736842105, '160309329X'),\n",
       " (0.012987012987012988, 'B003N5VTUY'),\n",
       " (0.012987012987012988, 'B000028D3Y')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostSimilar(query, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity based Recommendations(top 10)\n",
      "['1401207529', '1401232590', '1401223176', '1401235425', 'B00X5ZT04E', '140120841X', '1607066017', '160309329X', 'B003N5VTUY', 'B000028D3Y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Batman Year One Deluxe SC',\n",
       " 'Batman The Long Halloween TP',\n",
       " 'Batman Hush Complete TP',\n",
       " 'Batman Volume 1: The Court of Owls TP (The New 52) (Batman (DC Comics Paperback))',\n",
       " 'X-Men: Days of Future Past - Rogue Cut [Blu-ray] [2014]',\n",
       " 'V For Vendetta New Edition TP',\n",
       " 'Saga Volume 1 (Saga (Comic Series))',\n",
       " 'The League of Extraordinary Gentlemen (Volume III): Century|The League of Extraordinary Gentlemen|The League of Extraordinary Gentlemen',\n",
       " 'Batman: Under the Red Hood',\n",
       " 'Instrument (Soundtrack)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Similarity based Recommendations(top 10)\")\n",
    "[itemNames[x[1]] for x in mostSimilar(query, 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments:\n",
    "1. MSE after optimizing has slightly better result than average rating.\n",
    "2. Recommendations on products was done here; Predicting ratings can be done further.\n",
    "3. For the input product of Batman The Killing Joke Special Ed HC you get a list of comic related recommendations, except for Instrument which is a music category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
